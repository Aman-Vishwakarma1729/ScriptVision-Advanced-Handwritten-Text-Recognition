{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from typing import Tuple\n",
    "import cv2\n",
    "import lmdb\n",
    "import numpy as np\n",
    "from path import Path\n",
    "import random\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from typing import Tuple, List\n",
    "import editdistance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderIAM:\n",
    "\n",
    "    \n",
    "    Sample = namedtuple('Sample', 'gt_text, file_path')\n",
    "    Batch = namedtuple('Batch', 'imgs, gt_texts, batch_size')\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads data which corresponds to IAM format,\n",
    "    see: http://www.fki.inf.unibe.ch/databases/iam-handwriting-database\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: Path,\n",
    "                 batch_size: int,\n",
    "                 data_split: float = 0.95,\n",
    "                 fast: bool = True) -> None:\n",
    "        \"\"\"Loader for dataset.\"\"\"\n",
    "        \n",
    "        assert data_dir.exists()\n",
    "\n",
    "        self.fast = fast\n",
    "        if fast:\n",
    "            self.env = lmdb.open(str(data_dir / 'lmdb'), readonly=True)\n",
    "\n",
    "        self.data_augmentation = False\n",
    "        self.curr_idx = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.samples = []\n",
    "\n",
    "        with open(data_dir / 'gt/words.txt') as f:\n",
    "            chars = set()\n",
    "            bad_samples_reference = ['a01-117-05-02', 'r06-022-03-05']  # known broken images in IAM dataset\n",
    "            for line in f:\n",
    "                # ignore empty and comment lines\n",
    "                line = line.strip()\n",
    "                if not line or line[0] == '#':\n",
    "                    continue\n",
    "\n",
    "                line_split = line.split(' ')\n",
    "                assert len(line_split) >= 9\n",
    "\n",
    "                # filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
    "                file_name_split = line_split[0].split('-')\n",
    "                file_name_subdir1 = file_name_split[0]\n",
    "                file_name_subdir2 = f'{file_name_split[0]}-{file_name_split[1]}'\n",
    "                file_base_name = line_split[0] + '.png'\n",
    "                file_name = data_dir / 'img' / file_name_subdir1 / file_name_subdir2 / file_base_name\n",
    "\n",
    "                if line_split[0] in bad_samples_reference:\n",
    "                    print('Ignoring known broken image:', file_name)\n",
    "                    continue\n",
    "\n",
    "                # GT text are columns starting at 9\n",
    "                gt_text = ' '.join(line_split[8:])\n",
    "                chars = chars.union(set(list(gt_text)))\n",
    "\n",
    "                # put sample into list\n",
    "                self.samples.append(Sample(gt_text, file_name))\n",
    "\n",
    "        # split into training and validation set: 95% - 5%\n",
    "        split_idx = int(data_split * len(self.samples))\n",
    "        self.train_samples = self.samples[:split_idx]\n",
    "        self.validation_samples = self.samples[split_idx:]\n",
    "\n",
    "        # put words into lists\n",
    "        self.train_words = [x.gt_text for x in self.train_samples]\n",
    "        self.validation_words = [x.gt_text for x in self.validation_samples]\n",
    "\n",
    "        # start with train set\n",
    "        self.train_set()\n",
    "\n",
    "        # list of all chars in dataset\n",
    "        self.char_list = sorted(list(chars))\n",
    "\n",
    "    def train_set(self) -> None:\n",
    "        \"\"\"Switch to randomly chosen subset of training set.\"\"\"\n",
    "        self.data_augmentation = True\n",
    "        self.curr_idx = 0\n",
    "        random.shuffle(self.train_samples)\n",
    "        self.samples = self.train_samples\n",
    "        self.curr_set = 'train'\n",
    "\n",
    "    def validation_set(self) -> None:\n",
    "        \"\"\"Switch to validation set.\"\"\"\n",
    "        self.data_augmentation = False\n",
    "        self.curr_idx = 0\n",
    "        self.samples = self.validation_samples\n",
    "        self.curr_set = 'val'\n",
    "\n",
    "    def get_iterator_info(self) -> Tuple[int, int]:\n",
    "        \"\"\"Current batch index and overall number of batches.\"\"\"\n",
    "        if self.curr_set == 'train':\n",
    "            num_batches = int(np.floor(len(self.samples) / self.batch_size))  # train set: only full-sized batches\n",
    "        else:\n",
    "            num_batches = int(np.ceil(len(self.samples) / self.batch_size))  # val set: allow last batch to be smaller\n",
    "        curr_batch = self.curr_idx // self.batch_size + 1\n",
    "        return curr_batch, num_batches\n",
    "\n",
    "    def has_next(self) -> bool:\n",
    "        \"\"\"Is there a next element?\"\"\"\n",
    "        if self.curr_set == 'train':\n",
    "            return self.curr_idx + self.batch_size <= len(self.samples)  # train set: only full-sized batches\n",
    "        else:\n",
    "            return self.curr_idx < len(self.samples)  # val set: allow last batch to be smaller\n",
    "\n",
    "    def _get_img(self, i: int) -> np.ndarray:\n",
    "        if self.fast:\n",
    "            with self.env.begin() as txn:\n",
    "                basename = Path(self.samples[i].file_path).name\n",
    "                data = txn.get(basename.encode(\"ascii\"))\n",
    "                img = pickle.loads(data)\n",
    "        else:\n",
    "            img = cv2.imread(str(self.samples[i].file_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def get_next(self) -> Batch:\n",
    "        \"\"\"Get next element.\"\"\"\n",
    "        batch_range = range(self.curr_idx, min(self.curr_idx + self.batch_size, len(self.samples)))\n",
    "\n",
    "        imgs = [self._get_img(i) for i in batch_range]\n",
    "        gt_texts = [self.samples[i].gt_text for i in batch_range]\n",
    "\n",
    "        self.curr_idx += self.batch_size\n",
    "        return Batch(imgs, gt_texts, len(imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to read the image file.\n"
     ]
    }
   ],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self,\n",
    "                 img_size: Tuple[int, int],\n",
    "                 padding: int = 0,\n",
    "                 dynamic_width: bool = False,\n",
    "                 data_augmentation: bool = False,\n",
    "                 line_mode: bool = False) -> None:\n",
    "        # dynamic width only supported when no data augmentation happens\n",
    "        assert not (dynamic_width and data_augmentation)\n",
    "        # when padding is on, we need dynamic width enabled\n",
    "        assert not (padding > 0 and not dynamic_width)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.padding = padding\n",
    "        self.dynamic_width = dynamic_width\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.line_mode = line_mode\n",
    "\n",
    "    @staticmethod\n",
    "    def _truncate_label(text: str, max_text_len: int) -> str:\n",
    "        \"\"\"\n",
    "        Function ctc_loss can't compute loss if it cannot find a mapping between text label and input\n",
    "        labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
    "        If a too-long label is provided, ctc_loss returns an infinite gradient.\n",
    "        \"\"\"\n",
    "        cost = 0\n",
    "        for i in range(len(text)):\n",
    "            if i != 0 and text[i] == text[i - 1]:\n",
    "                cost += 2\n",
    "            else:\n",
    "                cost += 1\n",
    "            if cost > max_text_len:\n",
    "                return text[:i]\n",
    "        return text\n",
    "\n",
    "    def _simulate_text_line(self, batch: Batch) -> Batch:\n",
    "        \"\"\"Create image of a text line by pasting multiple word images into an image.\"\"\"\n",
    "\n",
    "        default_word_sep = 30\n",
    "        default_num_words = 5\n",
    "\n",
    "        # go over all batch elements\n",
    "        res_imgs = []\n",
    "        res_gt_texts = []\n",
    "        for i in range(batch.batch_size):\n",
    "            # number of words to put into current line\n",
    "            num_words = random.randint(1, 8) if self.data_augmentation else default_num_words\n",
    "\n",
    "            # concat ground truth texts\n",
    "            curr_gt = ' '.join([batch.gt_texts[(i + j) % batch.batch_size] for j in range(num_words)])\n",
    "            res_gt_texts.append(curr_gt)\n",
    "\n",
    "            # put selected word images into list, compute target image size\n",
    "            sel_imgs = []\n",
    "            word_seps = [0]\n",
    "            h = 0\n",
    "            w = 0\n",
    "            for j in range(num_words):\n",
    "                curr_sel_img = batch.imgs[(i + j) % batch.batch_size]\n",
    "                curr_word_sep = random.randint(20, 50) if self.data_augmentation else default_word_sep\n",
    "                h = max(h, curr_sel_img.shape[0])\n",
    "                w += curr_sel_img.shape[1]\n",
    "                sel_imgs.append(curr_sel_img)\n",
    "                if j + 1 < num_words:\n",
    "                    w += curr_word_sep\n",
    "                    word_seps.append(curr_word_sep)\n",
    "\n",
    "            # put all selected word images into target image\n",
    "            target = np.ones([h, w], np.uint8) * 255\n",
    "            x = 0\n",
    "            for curr_sel_img, curr_word_sep in zip(sel_imgs, word_seps):\n",
    "                x += curr_word_sep\n",
    "                y = (h - curr_sel_img.shape[0]) // 2\n",
    "                target[y:y + curr_sel_img.shape[0]:, x:x + curr_sel_img.shape[1]] = curr_sel_img\n",
    "                x += curr_sel_img.shape[1]\n",
    "\n",
    "            # put image of line into result\n",
    "            res_imgs.append(target)\n",
    "\n",
    "        return Batch(res_imgs, res_gt_texts, batch.batch_size)\n",
    "\n",
    "    def process_img(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Resize to target size, apply data augmentation.\"\"\"\n",
    "\n",
    "        # there are damaged files in IAM dataset - just use black image instead\n",
    "        if img is None:\n",
    "            img = np.zeros(self.img_size[::-1])\n",
    "\n",
    "        # data augmentation\n",
    "        img = img.astype(float)\n",
    "        if self.data_augmentation:\n",
    "            # photometric data augmentation\n",
    "            if random.random() < 0.25:\n",
    "                def rand_odd():\n",
    "                    return random.randint(1, 3) * 2 + 1\n",
    "                img = cv2.GaussianBlur(img, (rand_odd(), rand_odd()), 0)\n",
    "            if random.random() < 0.25:\n",
    "                img = cv2.dilate(img, np.ones((3, 3)))\n",
    "            if random.random() < 0.25:\n",
    "                img = cv2.erode(img, np.ones((3, 3)))\n",
    "\n",
    "            # geometric data augmentation\n",
    "            wt, ht = self.img_size\n",
    "            h, w = img.shape\n",
    "            f = min(wt / w, ht / h)\n",
    "            fx = f * np.random.uniform(0.75, 1.05)\n",
    "            fy = f * np.random.uniform(0.75, 1.05)\n",
    "\n",
    "            # random position around center\n",
    "            txc = (wt - w * fx) / 2\n",
    "            tyc = (ht - h * fy) / 2\n",
    "            freedom_x = max((wt - fx * w) / 2, 0)\n",
    "            freedom_y = max((ht - fy * h) / 2, 0)\n",
    "            tx = txc + np.random.uniform(-freedom_x, freedom_x)\n",
    "            ty = tyc + np.random.uniform(-freedom_y, freedom_y)\n",
    "\n",
    "            # map image into target image\n",
    "            M = np.float32([[fx, 0, tx], [0, fy, ty]])\n",
    "            target = np.ones(self.img_size[::-1]) * 255\n",
    "            img = cv2.warpAffine(img, M, dsize=self.img_size, dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
    "\n",
    "            # photometric data augmentation\n",
    "            if random.random() < 0.5:\n",
    "                img = img * (0.25 + random.random() * 0.75)\n",
    "            if random.random() < 0.25:\n",
    "                img = np.clip(img + (np.random.random(img.shape) - 0.5) * random.randint(1, 25), 0, 255)\n",
    "            if random.random() < 0.1:\n",
    "                img = 255 - img\n",
    "\n",
    "        # no data augmentation\n",
    "        else:\n",
    "            if self.dynamic_width:\n",
    "                ht = self.img_size[1]\n",
    "                h, w = img.shape\n",
    "                f = ht / h\n",
    "                wt = int(f * w + self.padding)\n",
    "                wt = wt + (4 - wt) % 4\n",
    "                tx = (wt - w * f) / 2\n",
    "                ty = 0\n",
    "            else:\n",
    "                wt, ht = self.img_size\n",
    "                h, w = img.shape\n",
    "                f = min(wt / w, ht / h)\n",
    "                tx = (wt - w * f) / 2\n",
    "                ty = (ht - h * f) / 2\n",
    "\n",
    "            # map image into target image\n",
    "            M = np.float32([[f, 0, tx], [0, f, ty]])\n",
    "            target = np.ones([ht, wt]) * 255\n",
    "            img = cv2.warpAffine(img, M, dsize=(wt, ht), dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
    "\n",
    "        # transpose for TF\n",
    "        img = cv2.transpose(img)\n",
    "\n",
    "        # convert to range [-1, 1]\n",
    "        img = img / 255 - 0.5\n",
    "        return img\n",
    "\n",
    "    def process_batch(self, batch: Batch) -> Batch:\n",
    "        if self.line_mode:\n",
    "            batch = self._simulate_text_line(batch)\n",
    "\n",
    "        res_imgs = [self.process_img(img) for img in batch.imgs]\n",
    "        max_text_len = res_imgs[0].shape[0] // 4\n",
    "        res_gt_texts = [self._truncate_label(gt_text, max_text_len) for gt_text in batch.gt_texts]\n",
    "        return Batch(res_imgs, res_gt_texts, batch.batch_size)\n",
    "\n",
    "\n",
    "def main():\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    img = cv2.imread('../data/test.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if img is None:\n",
    "        print(\"Error: Unable to read the image file.\")\n",
    "        return\n",
    "\n",
    "    img_aug = Preprocessor((256, 32), data_augmentation=True).process_img(img)\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(cv2.transpose(img_aug) + 0.5, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amanv\\AppData\\Local\\Temp\\ipykernel_7600\\1613833903.py:2: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\amanv\\AppData\\Local\\Temp\\ipykernel_7600\\1613833903.py:139: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Disable eager mode\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "class DecoderType:\n",
    "    \"\"\"CTC decoder types.\"\"\"\n",
    "    BestPath = 0\n",
    "    BeamSearch = 1\n",
    "    WordBeamSearch = 2\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"Minimalistic TF model for HTR.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 char_list: List[str],\n",
    "                 decoder_type: str = DecoderType.BestPath,\n",
    "                 must_restore: bool = False,\n",
    "                 dump: bool = False) -> None:\n",
    "        \"\"\"Init model: add CNN, RNN and CTC and initialize TF.\"\"\"\n",
    "        self.dump = dump\n",
    "        self.char_list = char_list\n",
    "        self.decoder_type = decoder_type\n",
    "        self.must_restore = must_restore\n",
    "        self.snap_ID = 0\n",
    "\n",
    "        # Whether to use normalization over a batch or a population\n",
    "        self.is_train = tf.compat.v1.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "        # input image batch\n",
    "        self.input_imgs = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None))\n",
    "\n",
    "        # setup CNN, RNN and CTC\n",
    "        self.setup_cnn()\n",
    "        self.setup_rnn()\n",
    "        self.setup_ctc()\n",
    "\n",
    "        # setup optimizer to train NN\n",
    "        self.batches_trained = 0\n",
    "        self.update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "            self.optimizer = tf.compat.v1.train.AdamOptimizer().minimize(self.loss)\n",
    "\n",
    "        # initialize TF\n",
    "        self.sess, self.saver = self.setup_tf()\n",
    "\n",
    "    def setup_cnn(self) -> None:\n",
    "        \"\"\"Create CNN layers.\"\"\"\n",
    "        cnn_in4d = tf.expand_dims(input=self.input_imgs, axis=3)\n",
    "\n",
    "        # list of parameters for the layers\n",
    "        kernel_vals = [5, 5, 3, 3, 3]\n",
    "        feature_vals = [1, 32, 64, 128, 128, 256]\n",
    "        stride_vals = pool_vals = [(2, 2), (2, 2), (1, 2), (1, 2), (1, 2)]\n",
    "        num_layers = len(stride_vals)\n",
    "\n",
    "        # create layers\n",
    "        pool = cnn_in4d  # input to first CNN layer\n",
    "        for i in range(num_layers):\n",
    "            kernel = tf.Variable(\n",
    "                tf.random.truncated_normal([kernel_vals[i], kernel_vals[i], feature_vals[i], feature_vals[i + 1]],\n",
    "                                           stddev=0.1))\n",
    "            conv = tf.nn.conv2d(input=pool, filters=kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
    "            conv_norm = tf.compat.v1.layers.batch_normalization(conv, training=self.is_train)\n",
    "            relu = tf.nn.relu(conv_norm)\n",
    "            pool = tf.nn.max_pool2d(input=relu, ksize=(1, pool_vals[i][0], pool_vals[i][1], 1),\n",
    "                                    strides=(1, stride_vals[i][0], stride_vals[i][1], 1), padding='VALID')\n",
    "\n",
    "        self.cnn_out_4d = pool\n",
    "\n",
    "    def setup_rnn(self) -> None:\n",
    "        \"\"\"Create RNN layers.\"\"\"\n",
    "        rnn_in3d = tf.squeeze(self.cnn_out_4d, axis=[2])\n",
    "\n",
    "        # basic cells which is used to build RNN\n",
    "        num_hidden = 256\n",
    "        cells = [tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=num_hidden, state_is_tuple=True) for _ in\n",
    "                 range(2)]  # 2 layers\n",
    "\n",
    "        # stack basic cells\n",
    "        stacked = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "        # bidirectional RNN\n",
    "        # BxTxF -> BxTx2H\n",
    "        (fw, bw), _ = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnn_in3d,\n",
    "                                                                dtype=rnn_in3d.dtype)\n",
    "\n",
    "        # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
    "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
    "\n",
    "        # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
    "        kernel = tf.Variable(tf.random.truncated_normal([1, 1, num_hidden * 2, len(self.char_list) + 1], stddev=0.1))\n",
    "        self.rnn_out_3d = tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'),\n",
    "                                     axis=[2])\n",
    "\n",
    "    def setup_ctc(self) -> None:\n",
    "        \"\"\"Create CTC loss and decoder.\"\"\"\n",
    "        # BxTxC -> TxBxC\n",
    "        self.ctc_in_3d_tbc = tf.transpose(a=self.rnn_out_3d, perm=[1, 0, 2])\n",
    "        # ground truth text as sparse tensor\n",
    "        self.gt_texts = tf.SparseTensor(tf.compat.v1.placeholder(tf.int64, shape=[None, 2]),\n",
    "                                        tf.compat.v1.placeholder(tf.int32, [None]),\n",
    "                                        tf.compat.v1.placeholder(tf.int64, [2]))\n",
    "\n",
    "        # calc loss for batch\n",
    "        self.seq_len = tf.compat.v1.placeholder(tf.int32, [None])\n",
    "        self.loss = tf.reduce_mean(\n",
    "            input_tensor=tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.ctc_in_3d_tbc,\n",
    "                                                  sequence_length=self.seq_len,\n",
    "                                                  ctc_merge_repeated=True))\n",
    "\n",
    "        # calc loss for each element to compute label probability\n",
    "        self.saved_ctc_input = tf.compat.v1.placeholder(tf.float32,\n",
    "                                                        shape=[None, None, len(self.char_list) + 1])\n",
    "        self.loss_per_element = tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.saved_ctc_input,\n",
    "                                                         sequence_length=self.seq_len, ctc_merge_repeated=True)\n",
    "\n",
    "        # best path decoding or beam search decoding\n",
    "        if self.decoder_type == DecoderType.BestPath:\n",
    "            self.decoder = tf.nn.ctc_greedy_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len)\n",
    "        elif self.decoder_type == DecoderType.BeamSearch:\n",
    "            self.decoder = tf.nn.ctc_beam_search_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len,\n",
    "                                                         beam_width=50)\n",
    "        # word beam search decoding (see https://github.com/githubharald/CTCWordBeamSearch)\n",
    "        elif self.decoder_type == DecoderType.WordBeamSearch:\n",
    "            # prepare information about language (dictionary, characters in dataset, characters forming words)\n",
    "            chars = ''.join(self.char_list)\n",
    "            word_chars = open('../model/wordCharList.txt').read().splitlines()[0]\n",
    "            corpus = open('../data/corpus.txt').read()\n",
    "\n",
    "            # decode using the \"Words\" mode of word beam search\n",
    "            from word_beam_search import WordBeamSearch\n",
    "            self.decoder = WordBeamSearch(50, 'Words', 0.0, corpus.encode('utf8'), chars.encode('utf8'),\n",
    "                                          word_chars.encode('utf8'))\n",
    "\n",
    "            # the input to the decoder must have softmax already applied\n",
    "            self.wbs_input = tf.nn.softmax(self.ctc_in_3d_tbc, axis=2)\n",
    "\n",
    "    def setup_tf(self) -> Tuple[tf.compat.v1.Session, tf.compat.v1.train.Saver]:\n",
    "        \"\"\"Initialize TF.\"\"\"\n",
    "        print('Python: ' + sys.version)\n",
    "        print('Tensorflow: ' + tf.__version__)\n",
    "\n",
    "        sess = tf.compat.v1.Session()  # TF session\n",
    "\n",
    "        saver = tf.compat.v1.train.Saver(max_to_keep=1)  # saver saves model to file\n",
    "        model_dir = '../model/'\n",
    "        latest_snapshot = tf.train.latest_checkpoint(model_dir)  # is there a saved model?\n",
    "\n",
    "        # if model must be restored (for inference), there must be a snapshot\n",
    "        if self.must_restore and not latest_snapshot:\n",
    "            raise Exception('No saved model found in: ' + model_dir)\n",
    "\n",
    "        # load saved model if available\n",
    "        if latest_snapshot:\n",
    "            print('Init with stored values from ' + latest_snapshot)\n",
    "            saver.restore(sess, latest_snapshot)\n",
    "        else:\n",
    "            print('Init with new values')\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        return sess, saver\n",
    "\n",
    "    def to_sparse(self, texts: List[str]) -> Tuple[List[List[int]], List[int], List[int]]:\n",
    "        \"\"\"Put ground truth texts into sparse tensor for ctc_loss.\"\"\"\n",
    "        indices = []\n",
    "        values = []\n",
    "        shape = [len(texts), 0]  # last entry must be max(labelList[i])\n",
    "\n",
    "        # go over all texts\n",
    "        for batchElement, text in enumerate(texts):\n",
    "            # convert to string of label (i.e. class-ids)\n",
    "            label_str = [self.char_list.index(c) for c in text]\n",
    "            # sparse tensor must have size of max. label-string\n",
    "            if len(label_str) > shape[1]:\n",
    "                shape[1] = len(label_str)\n",
    "            # put each label into sparse tensor\n",
    "            for i, label in enumerate(label_str):\n",
    "                indices.append([batchElement, i])\n",
    "                values.append(label)\n",
    "\n",
    "        return indices, values, shape\n",
    "\n",
    "    def decoder_output_to_text(self, ctc_output: tuple, batch_size: int) -> List[str]:\n",
    "        \"\"\"Extract texts from output of CTC decoder.\"\"\"\n",
    "\n",
    "        # word beam search: already contains label strings\n",
    "        if self.decoder_type == DecoderType.WordBeamSearch:\n",
    "            label_strs = ctc_output\n",
    "\n",
    "        # TF decoders: label strings are contained in sparse tensor\n",
    "        else:\n",
    "            # ctc returns tuple, first element is SparseTensor\n",
    "            decoded = ctc_output[0][0]\n",
    "\n",
    "            # contains string of labels for each batch element\n",
    "            label_strs = [[] for _ in range(batch_size)]\n",
    "\n",
    "            # go over all indices and save mapping: batch -> values\n",
    "            for (idx, idx2d) in enumerate(decoded.indices):\n",
    "                label = decoded.values[idx]\n",
    "                batch_element = idx2d[0]  # index according to [b,t]\n",
    "                label_strs[batch_element].append(label)\n",
    "\n",
    "        # map labels to chars for all batch elements\n",
    "        return [''.join([self.char_list[c] for c in labelStr]) for labelStr in label_strs]\n",
    "\n",
    "    def train_batch(self, batch: Batch) -> float:\n",
    "        \"\"\"Feed a batch into the NN to train it.\"\"\"\n",
    "        num_batch_elements = len(batch.imgs)\n",
    "        max_text_len = batch.imgs[0].shape[0] // 4\n",
    "        sparse = self.to_sparse(batch.gt_texts)\n",
    "        eval_list = [self.optimizer, self.loss]\n",
    "        feed_dict = {self.input_imgs: batch.imgs, self.gt_texts: sparse,\n",
    "                     self.seq_len: [max_text_len] * num_batch_elements, self.is_train: True}\n",
    "        _, loss_val = self.sess.run(eval_list, feed_dict)\n",
    "        self.batches_trained += 1\n",
    "        return loss_val\n",
    "\n",
    "    @staticmethod\n",
    "    def dump_nn_output(rnn_output: np.ndarray) -> None:\n",
    "        \"\"\"Dump the output of the NN to CSV file(s).\"\"\"\n",
    "        dump_dir = '../dump/'\n",
    "        if not os.path.isdir(dump_dir):\n",
    "            os.mkdir(dump_dir)\n",
    "\n",
    "        # iterate over all batch elements and create a CSV file for each one\n",
    "        max_t, max_b, max_c = rnn_output.shape\n",
    "        for b in range(max_b):\n",
    "            csv = ''\n",
    "            for t in range(max_t):\n",
    "                csv += ';'.join([str(rnn_output[t, b, c]) for c in range(max_c)]) + ';\\n'\n",
    "            fn = dump_dir + 'rnnOutput_' + str(b) + '.csv'\n",
    "            print('Write dump of NN to file: ' + fn)\n",
    "            with open(fn, 'w') as f:\n",
    "                f.write(csv)\n",
    "\n",
    "    def infer_batch(self, batch: Batch, calc_probability: bool = False, probability_of_gt: bool = False):\n",
    "        \"\"\"Feed a batch into the NN to recognize the texts.\"\"\"\n",
    "\n",
    "        # decode, optionally save RNN output\n",
    "        num_batch_elements = len(batch.imgs)\n",
    "\n",
    "        # put tensors to be evaluated into list\n",
    "        eval_list = []\n",
    "\n",
    "        if self.decoder_type == DecoderType.WordBeamSearch:\n",
    "            eval_list.append(self.wbs_input)\n",
    "        else:\n",
    "            eval_list.append(self.decoder)\n",
    "\n",
    "        if self.dump or calc_probability:\n",
    "            eval_list.append(self.ctc_in_3d_tbc)\n",
    "\n",
    "        # sequence length depends on input image size (model downsizes width by 4)\n",
    "        max_text_len = batch.imgs[0].shape[0] // 4\n",
    "\n",
    "        # dict containing all tensor fed into the model\n",
    "        feed_dict = {self.input_imgs: batch.imgs, self.seq_len: [max_text_len] * num_batch_elements,\n",
    "                     self.is_train: False}\n",
    "\n",
    "        # evaluate model\n",
    "        eval_res = self.sess.run(eval_list, feed_dict)\n",
    "\n",
    "        # TF decoders: decoding already done in TF graph\n",
    "        if self.decoder_type != DecoderType.WordBeamSearch:\n",
    "            decoded = eval_res[0]\n",
    "        # word beam search decoder: decoding is done in C++ function compute()\n",
    "        else:\n",
    "            decoded = self.decoder.compute(eval_res[0])\n",
    "\n",
    "        # map labels (numbers) to character string\n",
    "        texts = self.decoder_output_to_text(decoded, num_batch_elements)\n",
    "\n",
    "        # feed RNN output and recognized text into CTC loss to compute labeling probability\n",
    "        probs = None\n",
    "        if calc_probability:\n",
    "            sparse = self.to_sparse(batch.gt_texts) if probability_of_gt else self.to_sparse(texts)\n",
    "            ctc_input = eval_res[1]\n",
    "            eval_list = self.loss_per_element\n",
    "            feed_dict = {self.saved_ctc_input: ctc_input, self.gt_texts: sparse,\n",
    "                         self.seq_len: [max_text_len] * num_batch_elements, self.is_train: False}\n",
    "            loss_vals = self.sess.run(eval_list, feed_dict)\n",
    "            probs = np.exp(-loss_vals)\n",
    "\n",
    "        # dump the output of the NN to CSV file(s)\n",
    "        if self.dump:\n",
    "            self.dump_nn_output(eval_res[1])\n",
    "\n",
    "        return texts, probs\n",
    "\n",
    "    def save(self) -> None:\n",
    "        \"\"\"Save model to file.\"\"\"\n",
    "        self.snap_ID += 1\n",
    "        self.saver.save(self.sess, '../model/snapshot', global_step=self.snap_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --data_dir DATA_DIR\n",
      "ipykernel_launcher.py: error: the following arguments are required: --data_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=Path, required=True)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 2GB is enough for IAM dataset\n",
    "assert not (args.data_dir / 'lmdb').exists()\n",
    "env = lmdb.open(str(args.data_dir / 'lmdb'), map_size=1024 * 1024 * 1024 * 2)\n",
    "\n",
    "# go over all png files\n",
    "fn_imgs = list((args.data_dir / 'img').walkfiles('*.png'))\n",
    "\n",
    "# and put the imgs into lmdb as pickled grayscale imgs\n",
    "with env.begin(write=True) as txn:\n",
    "    for i, fn_img in enumerate(fn_imgs):\n",
    "        print(i, len(fn_imgs))\n",
    "        img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n",
    "        basename = fn_img.basename()\n",
    "        txn.put(basename.encode(\"ascii\"), pickle.dumps(img))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--mode {train,validate,infer}]\n",
      "                             [--decoder {bestpath,beamsearch,wordbeamsearch}]\n",
      "                             [--batch_size BATCH_SIZE] [--data_dir DATA_DIR]\n",
      "                             [--fast] [--line_mode] [--img_file IMG_FILE]\n",
      "                             [--early_stopping EARLY_STOPPING] [--dump]\n",
      "ipykernel_launcher.py: error: argument --fast: ignored explicit argument 'c:\\\\Users\\\\amanv\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-8200TEATzaPZfBb0.json'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\argparse.py:1895\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1895\u001b[0m     namespace, args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1896\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\argparse.py:2103\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[1;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[0;32m   2102\u001b[0m     \u001b[38;5;66;03m# consume the next optional and any arguments for it\u001b[39;00m\n\u001b[1;32m-> 2103\u001b[0m     start_index \u001b[38;5;241m=\u001b[39m \u001b[43mconsume_optional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;66;03m# consume any positionals following the last Optional\u001b[39;00m\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\argparse.py:2025\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_optional\u001b[1;34m(start_index)\u001b[0m\n\u001b[0;32m   2024\u001b[0m         msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignored explicit argument \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 2025\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ArgumentError(action, msg \u001b[38;5;241m%\u001b[39m explicit_arg)\n\u001b[0;32m   2027\u001b[0m \u001b[38;5;66;03m# if there is no explicit argument, try to match the\u001b[39;00m\n\u001b[0;32m   2028\u001b[0m \u001b[38;5;66;03m# optional's string arguments with the following strings\u001b[39;00m\n\u001b[0;32m   2029\u001b[0m \u001b[38;5;66;03m# if successful, exit the loop\u001b[39;00m\n\u001b[0;32m   2030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument --fast: ignored explicit argument 'c:\\\\Users\\\\amanv\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-8200TEATzaPZfBb0.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[40], line 196\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 196\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 158\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# parse arguments and set CTC decoder\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m decoder_mapping \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbestpath\u001b[39m\u001b[38;5;124m'\u001b[39m: DecoderType\u001b[38;5;241m.\u001b[39mBestPath,\n\u001b[0;32m    160\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeamsearch\u001b[39m\u001b[38;5;124m'\u001b[39m: DecoderType\u001b[38;5;241m.\u001b[39mBeamSearch,\n\u001b[0;32m    161\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordbeamsearch\u001b[39m\u001b[38;5;124m'\u001b[39m: DecoderType\u001b[38;5;241m.\u001b[39mWordBeamSearch}\n",
      "Cell \u001b[1;32mIn[40], line 151\u001b[0m, in \u001b[0;36mparse_args\u001b[1;34m()\u001b[0m\n\u001b[0;32m    149\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--dump\u001b[39m\u001b[38;5;124m'\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDump output of NN to CSV file(s).\u001b[39m\u001b[38;5;124m'\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\argparse.py:1862\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1862\u001b[0m     args, argv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argv:\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\argparse.py:1897\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1896\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1897\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\argparse.py:2617\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2616\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[1;32m-> 2617\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\argparse.py:2604\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2603\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m-> 2604\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2145\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2143\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2145\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2146\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2149\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:1454\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:1345\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1342\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:1192\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1185\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1190\u001b[0m ):\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1192\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:1082\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1080\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1081\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1082\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1083\u001b[0m )\n\u001b[0;32m   1085\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1086\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\External_Projects\\ScriptVision_Advanced_Handwritten_Text_Recognition\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:1150\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1150\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[0;32m   1151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1152\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "class FilePaths:\n",
    "    \"\"\"Filenames and paths to data.\"\"\"\n",
    "    fn_char_list = '../model/charList.txt'\n",
    "    fn_summary = '../model/summary.json'\n",
    "    fn_corpus = '../data/corpus.txt'\n",
    "\n",
    "\n",
    "def get_img_height() -> int:\n",
    "    \"\"\"Fixed height for NN.\"\"\"\n",
    "    return 32\n",
    "\n",
    "\n",
    "def get_img_size(line_mode: bool = False) -> Tuple[int, int]:\n",
    "    \"\"\"Height is fixed for NN, width is set according to training mode (single words or text lines).\"\"\"\n",
    "    if line_mode:\n",
    "        return 256, get_img_height()\n",
    "    return 128, get_img_height()\n",
    "\n",
    "\n",
    "def write_summary(average_train_loss: List[float], char_error_rates: List[float], word_accuracies: List[float]) -> None:\n",
    "    \"\"\"Writes training summary file for NN.\"\"\"\n",
    "    with open(FilePaths.fn_summary, 'w') as f:\n",
    "        json.dump({'averageTrainLoss': average_train_loss, 'charErrorRates': char_error_rates, 'wordAccuracies': word_accuracies}, f)\n",
    "\n",
    "\n",
    "def char_list_from_file() -> List[str]:\n",
    "    with open(FilePaths.fn_char_list) as f:\n",
    "        return list(f.read())\n",
    "\n",
    "\n",
    "def train(model: Model,\n",
    "          loader: DataLoaderIAM,\n",
    "          line_mode: bool,\n",
    "          early_stopping: int = 25) -> None:\n",
    "    \"\"\"Trains NN.\"\"\"\n",
    "    epoch = 0  # number of training epochs since start\n",
    "    summary_char_error_rates = []\n",
    "    summary_word_accuracies = []\n",
    "\n",
    "    train_loss_in_epoch = []\n",
    "    average_train_loss = []\n",
    "\n",
    "    preprocessor = Preprocessor(get_img_size(line_mode), data_augmentation=True, line_mode=line_mode)\n",
    "    best_char_error_rate = float('inf')  # best validation character error rate\n",
    "    no_improvement_since = 0  # number of epochs no improvement of character error rate occurred\n",
    "    # stop training after this number of epochs without improvement\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        print('Epoch:', epoch)\n",
    "\n",
    "        # train\n",
    "        print('Train NN')\n",
    "        loader.train_set()\n",
    "        while loader.has_next():\n",
    "            iter_info = loader.get_iterator_info()\n",
    "            batch = loader.get_next()\n",
    "            batch = preprocessor.process_batch(batch)\n",
    "            loss = model.train_batch(batch)\n",
    "            print(f'Epoch: {epoch} Batch: {iter_info[0]}/{iter_info[1]} Loss: {loss}')\n",
    "            train_loss_in_epoch.append(loss)\n",
    "\n",
    "        # validate\n",
    "        char_error_rate, word_accuracy = validate(model, loader, line_mode)\n",
    "\n",
    "        # write summary\n",
    "        summary_char_error_rates.append(char_error_rate)\n",
    "        summary_word_accuracies.append(word_accuracy)\n",
    "        average_train_loss.append((sum(train_loss_in_epoch)) / len(train_loss_in_epoch))\n",
    "        write_summary(average_train_loss, summary_char_error_rates, summary_word_accuracies)\n",
    "\n",
    "        # reset train loss list\n",
    "        train_loss_in_epoch = []\n",
    "\n",
    "        # if best validation accuracy so far, save model parameters\n",
    "        if char_error_rate < best_char_error_rate:\n",
    "            print('Character error rate improved, save model')\n",
    "            best_char_error_rate = char_error_rate\n",
    "            no_improvement_since = 0\n",
    "            model.save()\n",
    "        else:\n",
    "            print(f'Character error rate not improved, best so far: {best_char_error_rate * 100.0}%')\n",
    "            no_improvement_since += 1\n",
    "\n",
    "        # stop training if no more improvement in the last x epochs\n",
    "        if no_improvement_since >= early_stopping:\n",
    "            print(f'No more improvement for {early_stopping} epochs. Training stopped.')\n",
    "            break\n",
    "\n",
    "\n",
    "def validate(model: Model, loader: DataLoaderIAM, line_mode: bool) -> Tuple[float, float]:\n",
    "    \"\"\"Validates NN.\"\"\"\n",
    "    print('Validate NN')\n",
    "    loader.validation_set()\n",
    "    preprocessor = Preprocessor(get_img_size(line_mode), line_mode=line_mode)\n",
    "    num_char_err = 0\n",
    "    num_char_total = 0\n",
    "    num_word_ok = 0\n",
    "    num_word_total = 0\n",
    "    while loader.has_next():\n",
    "        iter_info = loader.get_iterator_info()\n",
    "        print(f'Batch: {iter_info[0]} / {iter_info[1]}')\n",
    "        batch = loader.get_next()\n",
    "        batch = preprocessor.process_batch(batch)\n",
    "        recognized, _ = model.infer_batch(batch)\n",
    "\n",
    "        print('Ground truth -> Recognized')\n",
    "        for i in range(len(recognized)):\n",
    "            num_word_ok += 1 if batch.gt_texts[i] == recognized[i] else 0\n",
    "            num_word_total += 1\n",
    "            dist = editdistance.eval(recognized[i], batch.gt_texts[i])\n",
    "            num_char_err += dist\n",
    "            num_char_total += len(batch.gt_texts[i])\n",
    "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gt_texts[i] + '\"', '->',\n",
    "                  '\"' + recognized[i] + '\"')\n",
    "\n",
    "    # print validation result\n",
    "    char_error_rate = num_char_err / num_char_total\n",
    "    word_accuracy = num_word_ok / num_word_total\n",
    "    print(f'Character error rate: {char_error_rate * 100.0}%. Word accuracy: {word_accuracy * 100.0}%.')\n",
    "    return char_error_rate, word_accuracy\n",
    "\n",
    "\n",
    "def infer(model: Model, fn_img: Path) -> None:\n",
    "    \"\"\"Recognizes text in image provided by file path.\"\"\"\n",
    "    img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n",
    "    assert img is not None\n",
    "\n",
    "    preprocessor = Preprocessor(get_img_size(), dynamic_width=True, padding=16)\n",
    "    img = preprocessor.process_img(img)\n",
    "\n",
    "    batch = Batch([img], None, 1)\n",
    "    recognized, probability = model.infer_batch(batch, True)\n",
    "    print(f'Recognized: \"{recognized[0]}\"')\n",
    "    print(f'Probability: {probability[0]}')\n",
    "\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    \"\"\"Parses arguments from the command line.\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--mode', choices=['train', 'validate', 'infer'], default='infer')\n",
    "    parser.add_argument('--decoder', choices=['bestpath', 'beamsearch', 'wordbeamsearch'], default='bestpath')\n",
    "    parser.add_argument('--batch_size', help='Batch size.', type=int, default=100)\n",
    "    parser.add_argument('--data_dir', help='Directory containing IAM dataset.', type=Path, required=False)\n",
    "    parser.add_argument('--fast', help='Load samples from LMDB.', action='store_true')\n",
    "    parser.add_argument('--line_mode', help='Train to read text lines instead of single words.', action='store_true')\n",
    "    parser.add_argument('--img_file', help='Image used for inference.', type=Path, default='../data/word.png')\n",
    "    parser.add_argument('--early_stopping', help='Early stopping epochs.', type=int, default=25)\n",
    "    parser.add_argument('--dump', help='Dump output of NN to CSV file(s).', action='store_true')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function.\"\"\"\n",
    "\n",
    "    # parse arguments and set CTC decoder\n",
    "    args = parse_args()\n",
    "    decoder_mapping = {'bestpath': DecoderType.BestPath,\n",
    "                       'beamsearch': DecoderType.BeamSearch,\n",
    "                       'wordbeamsearch': DecoderType.WordBeamSearch}\n",
    "    decoder_type = decoder_mapping[args.decoder]\n",
    "\n",
    "    # train the model\n",
    "    if args.mode == 'train':\n",
    "        loader = DataLoaderIAM(args.data_dir, args.batch_size, fast=args.fast)\n",
    "\n",
    "        # when in line mode, take care to have a whitespace in the char list\n",
    "        char_list = loader.char_list\n",
    "        if args.line_mode and ' ' not in char_list:\n",
    "            char_list = [' '] + char_list\n",
    "\n",
    "        # save characters and words\n",
    "        with open(FilePaths.fn_char_list, 'w') as f:\n",
    "            f.write(''.join(char_list))\n",
    "\n",
    "        with open(FilePaths.fn_corpus, 'w') as f:\n",
    "            f.write(' '.join(loader.train_words + loader.validation_words))\n",
    "\n",
    "        model = Model(char_list, decoder_type)\n",
    "        train(model, loader, line_mode=args.line_mode, early_stopping=args.early_stopping)\n",
    "\n",
    "    # evaluate it on the validation set\n",
    "    elif args.mode == 'validate':\n",
    "        loader = DataLoaderIAM(args.data_dir, args.batch_size, fast=args.fast)\n",
    "        model = Model(char_list_from_file(), decoder_type, must_restore=True)\n",
    "        validate(model, loader, args.line_mode)\n",
    "\n",
    "    # infer text on test image\n",
    "    elif args.mode == 'infer':\n",
    "        model = Model(char_list_from_file(), decoder_type, must_restore=True, dump=args.dump)\n",
    "        infer(model, args.img_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
